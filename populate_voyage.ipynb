{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c052d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, unicodedata, tempfile, shutil\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField,\n",
    "    StringType, TimestampType, IntegerType, DecimalType\n",
    ")\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from supabase import create_client\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.keyvault.secrets import SecretClient\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "  .appName(\"WriteToSupabase\")\n",
    "  .master(\"local[*]\")\n",
    "  .config(\"spark.jars.packages\", \"org.postgresql:postgresql:42.7.4\")\n",
    "  .getOrCreate())\n",
    "\n",
    "\n",
    "csv_path = os.path.expanduser(\"~/Documents/query_params.csv\")\n",
    "\n",
    "\n",
    "df = spark.read.csv(str(csv_path), header=False, inferSchema=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1e99cf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔗 Connected to Supabase schema: voyage table: summary\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "vault_name = 'akv-1'\n",
    "secret_name = 'supabase-service-role-key'\n",
    "supabase_url = 'supabase-url'\n",
    "credential = DefaultAzureCredential()  # uses Managed Identity or env-creds\n",
    "clientSecret = SecretClient(vault_url=f\"https://{vault_name}.vault.azure.net\", credential=credential)\n",
    "key =  clientSecret.get_secret(secret_name).value\n",
    "url = clientSecret.get_secret(supabase_url).value\n",
    "\n",
    "\n",
    "client = create_client(url, key)\n",
    "\n",
    "SCHEMA = \"voyage\"\n",
    "TABLE = \"summary\"\n",
    "\n",
    "print(\"🔗 Connected to Supabase schema:\", SCHEMA, \"table:\", TABLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef3fe205",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Full voyage.summary schema (from your DDL)\n",
    "FULL_SCHEMA = StructType([\n",
    "    StructField(\"updated_at\",           TimestampType(), True),\n",
    "    StructField(\"id\",                   IntegerType(),   False),\n",
    "    StructField(\"voyage_bcr_usd\",       DecimalType(18,2), True),\n",
    "    StructField(\"voyage_date_voyagestart\", TimestampType(), True),\n",
    "    StructField(\"voyage_id\",            StringType(),    True),\n",
    "    StructField(\"voyage_duration_days\", DecimalType(10,4), True),\n",
    "    StructField(\"voyage_rate_usd_24hrgross\", DecimalType(18,2), True),\n",
    "    StructField(\"voyage_oh_usd\",        DecimalType(18,2), True),\n",
    "    StructField(\"voyage_hac_usd\",       DecimalType(18,2), True),\n",
    "    StructField(\"voyage_cve_usd\",       DecimalType(18,2), True),\n",
    "    StructField(\"voyage_bnkr_usd\",      DecimalType(18,2), True),\n",
    "    StructField(\"voyage_bnkr_desc\",     StringType(),    True),\n",
    "    StructField(\"voyage_chtr_usd_bal\",  DecimalType(18,2), True),\n",
    "    StructField(\"voyage_expense_usd\",   DecimalType(18,2), True),\n",
    "    StructField(\"voyage_expense_desc\",  StringType(),    True),\n",
    "    StructField(\"voyage_bnkr_usd_deduc\",DecimalType(18,2), True),\n",
    "    StructField(\"created_by\",           StringType(),    True),  # uuid → keep as text\n",
    "    StructField(\"created_at\",           TimestampType(), True),\n",
    "    StructField(\"updated_by\",           StringType(),    True),  # uuid → keep as text\n",
    "])\n",
    "\n",
    "def schema_for_select_cols(select_cols):\n",
    "    \"\"\"Return a StructType with only the selected columns, in the same order.\"\"\"\n",
    "    # Map field names to fields for quick lookup\n",
    "    field_map = {f.name: f for f in FULL_SCHEMA.fields}\n",
    "    fields = []\n",
    "    for c in select_cols:\n",
    "        if c in field_map:\n",
    "            fields.append(field_map[c])\n",
    "        else:\n",
    "            # If user asked for a non-schema column, treat as string\n",
    "            fields.append(StructField(c, StringType(), True))\n",
    "    return StructType(fields)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ecd0abec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV shape: 4 rows x 17 cols\n",
      "✅ Candidate marker columns (from anchors): [0, 2]\n",
      "✅ Found marker 'Header' at row=0, col=0, value='Header'\n",
      "✅ Found marker 'StartDate' at row=1, col=2, value='StartDate'\n",
      "✅ Found marker 'EndDate' at row=2, col=2, value='EndDate'\n",
      "✅ Found marker 'Voyage ID' at row=3, col=2, value='Voyage ID'\n",
      "Markers: Header (0, 0) StartDate (1, 2) EndDate (2, 2) Voyage ID (3, 2)\n",
      "Markers → Header@(0,0)  Start@(1,2)  End@(2,2)  VoyageID@(3,2)\n",
      "Header (output order): ['voyage_date_voyagestart', 'voyage_id', 'voyage_duration_days', 'voyage_rate_usd_24hrgross', 'voyage_oh_usd', 'voyage_hac_usd', 'voyage_bcr_usd', 'voyage_cve_usd', 'voyage_bnkr_usd', 'voyage_bnkr_desc', 'voyage_chtr_usd_bal', 'voyage_expense_usd', 'voyage_expense_desc', 'voyage_bnkr_usd_deduc']\n",
      "Query SELECT columns : ['voyage_date_voyagestart', 'voyage_id', 'voyage_duration_days', 'voyage_rate_usd_24hrgross', 'voyage_oh_usd', 'voyage_hac_usd', 'voyage_bcr_usd', 'voyage_cve_usd', 'voyage_bnkr_usd', 'voyage_bnkr_desc', 'voyage_chtr_usd_bal', 'voyage_expense_usd', 'voyage_expense_desc', 'voyage_bnkr_usd_deduc']\n",
      "Discovered 3 set(s):\n",
      "  01. voyage_id='250626/GTR/BAL/HAR/SIN', start=2025-07-03T00:00:00, end=2025-07-10T00:00:00\n",
      "  02. voyage_id='250810/GMA/BBA/LSH/BNS', start=2025-08-12T00:00:00, end=2025-08-17T00:00:00\n",
      "  03. voyage_id='250810/GMA/BBA/LSH/BNS', start=2025-10-04T00:00:00, end=2025-10-08T00:00:00\n",
      "SMOKE for first ID: 250626/GTR/BAL/HAR/SIN -> 3 rows\n",
      "Fetched raw rows (pre-dedup): 19\n",
      "✅ Wrote: C:\\Users\\youngw417/Documents/db_results.csv | rows: 19 | columns: ['voyage_date_voyagestart', 'voyage_id', 'voyage_duration_days', 'voyage_rate_usd_24hrgross', 'voyage_oh_usd', 'voyage_hac_usd', 'voyage_bcr_usd', 'voyage_cve_usd', 'voyage_bnkr_usd', 'voyage_bnkr_desc', 'voyage_chtr_usd_bal', 'voyage_expense_usd', 'voyage_expense_desc', 'voyage_bnkr_usd_deduc']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# ========= CONFIG =========\n",
    "SCHEMA = \"voyage\"\n",
    "TABLE  = \"summary\"\n",
    "DATE_COL = \"voyage_date_voyagestart\"\n",
    "USE_DATE_ONLY = False\n",
    "\n",
    "OUT_CSV = os.path.expanduser(\"~/Documents/db_results.csv\")\n",
    "\n",
    "\n",
    "SCHEMA_COLS = {\n",
    "    \"updated_at\",\"id\",\"voyage_bcr_usd\",\"voyage_date_voyagestart\",\"voyage_id\",\n",
    "    \"voyage_duration_days\",\"voyage_rate_usd_24hrgross\",\"voyage_oh_usd\",\"voyage_hac_usd\",\n",
    "    \"voyage_cve_usd\",\"voyage_bnkr_usd\",\"voyage_bnkr_desc\",\"voyage_chtr_usd_bal\",\n",
    "    \"voyage_expense_usd\",\"voyage_expense_desc\",\"voyage_bnkr_usd_deduc\",\"created_by\",\n",
    "    \"created_at\",\"updated_by\"\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "pdf = df.toPandas().fillna(\"\")\n",
    "\n",
    "n_rows, n_cols = pdf.shape\n",
    "print(\"CSV shape:\", n_rows, \"rows x\", n_cols, \"cols\")\n",
    "\n",
    "\n",
    "# ========= HELPERS =========\n",
    "def _norm(s: str) -> str:\n",
    "    \"\"\"Normalize text for matching markers (keep underscores).\"\"\"\n",
    "    s = str(s).strip().casefold()\n",
    "    s = re.sub(r\"\\s+\", \"_\", s)            # spaces -> underscore\n",
    "    return re.sub(r\"[^0-9a-z_]+\", \"\", s)  # keep a-z, 0-9, _\n",
    "\n",
    "def parse_to_iso(val, *, is_end=False):\n",
    "    \"\"\"Return 'YYYY-MM-DDTHH:MM:SS' or None. Date-only → 00:00:00 / 23:59:59.\"\"\"\n",
    "    if val is None: return None\n",
    "    s = str(val).strip()\n",
    "    if not s or s.upper() == \"NULL\": return None\n",
    "    if s.upper().startswith(\"UTC \"): s = s[4:].strip()\n",
    "    if s.endswith(\"Z\"): s = s[:-1]\n",
    "    s_space = s.replace(\"T\", \" \")\n",
    "    # full datetime\n",
    "    for fmt in (\"%Y-%m-%d %H:%M:%S\", \"%Y-%m-%d %H:%M\",\n",
    "                \"%m/%d/%Y %H:%M:%S\", \"%m/%d/%Y %H:%M\"):\n",
    "        try:\n",
    "            dt = datetime.strptime(s_space, fmt)\n",
    "            return dt.strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
    "        except ValueError:\n",
    "            pass\n",
    "    # date-only\n",
    "    for fmt in (\"%Y-%m-%d\", \"%m/%d/%Y\"):\n",
    "        try:\n",
    "            d = datetime.strptime(s_space, fmt)\n",
    "            return d.strftime(\"%Y-%m-%dT23:59:59\" if is_end else \"%Y-%m-%dT00:00:00\")\n",
    "        except ValueError:\n",
    "            pass\n",
    "    return None\n",
    "\n",
    "def clean_voyage_id(value):\n",
    "    \"\"\"Trim, normalize, collapse spaces; keep exact visible chars.\"\"\"\n",
    "    if value is None:\n",
    "        return None\n",
    "    s = str(value).strip().strip(\"'\").strip('\"')\n",
    "    if not s or s.upper() == \"NULL\":\n",
    "        return None\n",
    "    s = unicodedata.normalize(\"NFKC\", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    return s\n",
    "\n",
    "def discover_marker_cols(pdf, *, anchors=(\"Header\", \"StartDate\")) -> list[int]:\n",
    "    \"\"\"\n",
    "    Scan the sheet to find which columns contain any of the anchor markers\n",
    "    (by default 'Header' and 'StartDate'). Return a sorted, de-duped list of\n",
    "    column indices to use as candidate columns for all marker searches.\n",
    "    \"\"\"\n",
    "    want = {_norm(a) for a in anchors}\n",
    "    found_cols = set()\n",
    "    n_rows, n_cols = pdf.shape\n",
    "\n",
    "    for i in range(n_rows):\n",
    "        for j in range(n_cols):\n",
    "            v = pdf.iat[i, j]\n",
    "            if v is None:\n",
    "                continue\n",
    "            if _norm(v) in want:\n",
    "                found_cols.add(j)\n",
    "\n",
    "    cols = sorted(found_cols)\n",
    "    if not cols:\n",
    "        # Fallback: if anchors not found anywhere, search all columns\n",
    "        cols = list(range(n_cols))\n",
    "        print(\"⚠️ No anchor markers found; falling back to all columns.\")\n",
    "    else:\n",
    "        print(f\"✅ Candidate marker columns (from anchors): {cols}\")\n",
    "    return cols\n",
    "\n",
    "\n",
    "def find_marker_rc(pdf, marker: str, candidate_cols: list[int] | None = None):\n",
    "    \"\"\"\n",
    "    Find (row, col) for a marker cell, searching ONLY within candidate_cols.\n",
    "    If candidate_cols is None, they are discovered from 'Header'/'StartDate'.\n",
    "    Returns (row_index, col_index) or None if not found.\n",
    "    \"\"\"\n",
    "    if candidate_cols is None:\n",
    "        candidate_cols = discover_marker_cols(pdf, anchors=(\"Header\", \"StartDate\"))\n",
    "\n",
    "    tgt = _norm(marker)\n",
    "    n_rows, _ = pdf.shape\n",
    "\n",
    "    for i in range(n_rows):\n",
    "        for j in candidate_cols:\n",
    "            if j >= len(pdf.columns):  # safety\n",
    "                continue\n",
    "            v = pdf.iat[i, j]\n",
    "            if v is None:\n",
    "                continue\n",
    "            if _norm(v) == tgt:\n",
    "                print(f\"✅ Found marker '{marker}' at row={i}, col={j}, value='{v}'\")\n",
    "                return i, j\n",
    "\n",
    "    print(f\"⚠️ Marker '{marker}' not found in candidate columns {candidate_cols}.\")\n",
    "    return None\n",
    "\n",
    "def sanitize_header_names(cols: list[str]) -> list[str]:\n",
    "    \"\"\"Lowercase, replace hyphens/spaces with underscore; strip.\"\"\"\n",
    "    cleaned = []\n",
    "    for c in cols:\n",
    "        if c is None: \n",
    "            continue\n",
    "        s = str(c).strip()\n",
    "        if not s or s.upper() == \"NULL\":\n",
    "            continue\n",
    "        s = unicodedata.normalize(\"NFKC\", s)\n",
    "        s = s.lower()\n",
    "        s = re.sub(r\"[-\\s]+\", \"_\", s)\n",
    "        cleaned.append(s)\n",
    "    return cleaned\n",
    "\n",
    "cand_cols = discover_marker_cols(pdf)\n",
    "\n",
    "# ========= FIND MARKERS =========\n",
    "hdr_rc   = find_marker_rc(pdf, \"Header\",    candidate_cols=cand_cols)\n",
    "start_rc = find_marker_rc(pdf, \"StartDate\", candidate_cols=cand_cols)\n",
    "end_rc   = find_marker_rc(pdf, \"EndDate\",   candidate_cols=cand_cols)\n",
    "vid_rc   = find_marker_rc(pdf, \"Voyage ID\", candidate_cols=cand_cols)\n",
    "\n",
    "print(\"Markers:\",\n",
    "      \"Header\", hdr_rc,\n",
    "      \"StartDate\", start_rc,\n",
    "      \"EndDate\", end_rc,\n",
    "      \"Voyage ID\", vid_rc)\n",
    "\n",
    "if not hdr_rc:   raise ValueError(\"Header marker not found\")\n",
    "if not vid_rc:   raise ValueError(\"Voyage ID marker not found\")\n",
    "\n",
    "r_hdr,  c_hdr  = hdr_rc\n",
    "r_start, c_start = start_rc if start_rc else (None, None)\n",
    "r_end,   c_end   = end_rc   if end_rc   else (None, None)\n",
    "r_vid,   c_vid   = vid_rc\n",
    "\n",
    "print(f\"Markers → Header@({r_hdr},{c_hdr})  Start@({r_start},{c_start})  End@({r_end},{c_end})  VoyageID@({r_vid},{c_vid})\")\n",
    "\n",
    "\n",
    "# ========= BUILD HEADER COLUMNS (from Header row, to the right) =========\n",
    "header_cols_raw = []\n",
    "for j in range(c_hdr + 1, n_cols):\n",
    "    v = pdf.iat[r_hdr, j]\n",
    "    if v is None: \n",
    "        continue\n",
    "    s = str(v).strip()\n",
    "    if not s or s.upper() == \"NULL\":\n",
    "        continue\n",
    "    header_cols_raw.append(s)\n",
    "\n",
    "header_cols = sanitize_header_names(header_cols_raw)\n",
    "if not header_cols:\n",
    "    raise ValueError(\"Header row found but no DB column names to the right.\")\n",
    "\n",
    "# Build select list: header + keys for dedupe\n",
    "query_select_cols = header_cols.copy()\n",
    "for must in (\"voyage_id\", DATE_COL):\n",
    "    if must not in query_select_cols:\n",
    "        query_select_cols.append(must)\n",
    "\n",
    "print(\"Header (output order):\", header_cols)\n",
    "print(\"Query SELECT columns :\", query_select_cols)\n",
    "\n",
    "\n",
    "# ========= BUILD QUERY SETS PER COLUMN (to the right of markers, same rows) =========\n",
    "sets = []\n",
    "for j in range(c_vid + 1, n_cols):\n",
    "    vid_val = pdf.iat[r_vid, j] if r_vid is not None else None\n",
    "    vid = clean_voyage_id(vid_val)\n",
    "    if not vid:\n",
    "        continue\n",
    "\n",
    "    s_start = parse_to_iso(pdf.iat[r_start, j], is_end=False) if r_start is not None else None\n",
    "    s_end   = parse_to_iso(pdf.iat[r_end,   j], is_end=True)  if r_end   is not None else None\n",
    "\n",
    "    sets.append({\"id\": vid, \"start\": s_start, \"end\": s_end})\n",
    "\n",
    "print(f\"Discovered {len(sets)} set(s):\")\n",
    "for i, s in enumerate(sets, 1):\n",
    "    print(f\"  {i:02d}. voyage_id={repr(s['id'])}, start={s['start']}, end={s['end']}\")\n",
    "\n",
    "\n",
    "# ========= OPTIONAL: Smoke test the first ID to verify the DB actually has rows =========\n",
    "if sets:\n",
    "    test_id = sets[0][\"id\"]\n",
    "    smoke = client.schema(SCHEMA).table(TABLE) \\\n",
    "        .select(f\"voyage_id,{DATE_COL}\") \\\n",
    "        .eq(\"voyage_id\", test_id) \\\n",
    "        .limit(3).execute()\n",
    "    print(\"SMOKE for first ID:\", test_id, \"->\", len(smoke.data), \"rows\")\n",
    "    # If 0 here, the ID text doesn't exist in DB as-is (missing, spacing, or typo).\n",
    "\n",
    "\n",
    "# ========= RUN QUERIES & UNION RESULTS =========\n",
    "all_rows = []\n",
    "page_size = 2000\n",
    "\n",
    "for s in sets:\n",
    "    q = client.schema(SCHEMA).table(TABLE).select(\",\".join(query_select_cols))\n",
    "\n",
    "    # Date filters\n",
    "    if s[\"start\"]:\n",
    "        q = q.gte(DATE_COL, s[\"start\"][:10] if USE_DATE_ONLY else s[\"start\"])\n",
    "    if s[\"end\"]:\n",
    "        q = q.lte(DATE_COL, s[\"end\"][:10] if USE_DATE_ONLY else s[\"end\"])\n",
    "\n",
    "    # Voyage ID filter\n",
    "    q = q.eq(\"voyage_id\", s[\"id\"])\n",
    "\n",
    "    # Optional ordering\n",
    "    q = q.order(DATE_COL, desc=True, nullsfirst=False)\n",
    "\n",
    "    # Pagination\n",
    "    page = 0\n",
    "    while True:\n",
    "        resp = q.range(page * page_size, page * page_size + page_size - 1).execute()\n",
    "        batch = resp.data or []\n",
    "        all_rows.extend(batch)\n",
    "        if len(batch) < page_size:\n",
    "            break\n",
    "        page += 1\n",
    "\n",
    "print(\"Fetched raw rows (pre-dedup):\", len(all_rows))\n",
    "\n",
    "\n",
    "# ========= PANDAS: DEDUP & WRITE ONLY HEADER COLS =========\n",
    "if all_rows:\n",
    "    pd_df = pd.DataFrame(all_rows)\n",
    "else:\n",
    "    pd_df = pd.DataFrame(columns=query_select_cols)\n",
    "\n",
    "# De-dup on unique key pair\n",
    "if {\"voyage_id\", DATE_COL}.issubset(pd_df.columns):\n",
    "    pd_df = pd_df.drop_duplicates(subset=[\"voyage_id\", DATE_COL])\n",
    "elif \"voyage_id\" in pd_df.columns:\n",
    "    pd_df = pd_df.drop_duplicates(subset=[\"voyage_id\"])\n",
    "else:\n",
    "    pd_df = pd_df.drop_duplicates()\n",
    "\n",
    "\n",
    "if DATE_COL in pd_df.columns:\n",
    "    _SORT_COL = \"__sort_dt__\"\n",
    "    # Try to parse to datetime; rows that can’t parse go to the end\n",
    "    pd_df[_SORT_COL] = pd.to_datetime(pd_df[DATE_COL], errors=\"coerce\")\n",
    "    # Primary: date ascending; Secondary (optional): voyage_id for stability\n",
    "    if \"voyage_id\" in pd_df.columns:\n",
    "        pd_df = pd_df.sort_values(by=[_SORT_COL, \"voyage_id\"], ascending=[True, True], kind=\"stable\")\n",
    "    else:\n",
    "        pd_df = pd_df.sort_values(by=[_SORT_COL], ascending=True, kind=\"stable\")\n",
    "    pd_df = pd_df.drop(columns=[_SORT_COL], errors=\"ignore\")\n",
    "else:\n",
    "    print(f\"⚠️ Sort skipped: '{DATE_COL}' not in DataFrame columns:\", list(pd_df.columns))\n",
    "\n",
    "# Keep ONLY header columns (exact order); create missing cols if needed\n",
    "for c in header_cols:\n",
    "    if c not in pd_df.columns:\n",
    "        pd_df[c] = \"\"\n",
    "pd_df = pd_df[header_cols]\n",
    "\n",
    "# Write safely (avoid Excel lock failures)\n",
    "fd, tmp = tempfile.mkstemp(suffix=\".csv\", dir=os.path.dirname(OUT_CSV))\n",
    "os.close(fd)\n",
    "pd_df.to_csv(tmp, index=False, encoding=\"utf-8-sig\")\n",
    "try:\n",
    "    shutil.move(tmp, OUT_CSV)\n",
    "    print(\"✅ Wrote:\", OUT_CSV, \"| rows:\", len(pd_df), \"| columns:\", list(pd_df.columns))\n",
    "except PermissionError:\n",
    "    alt = OUT_CSV.replace(\".csv\", \"_new.csv\")\n",
    "    shutil.move(tmp, alt)\n",
    "    print(f\"⚠️ Excel lock detected — saved as '{alt}' instead. Rows: {len(pd_df)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "populate_voyage_v1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
