{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1c052d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "  .appName(\"WriteToSupabase\")\n",
    "  .master(\"local[*]\")\n",
    "  .config(\"spark.jars.packages\", \"org.postgresql:postgresql:42.7.4\")\n",
    "  .getOrCreate())\n",
    "\n",
    "\n",
    "csv_path = os.path.expanduser(\"~/Documents/populate_voyage/query_params.csv\")\n",
    "\n",
    "\n",
    "df = spark.read.csv(str(csv_path), header=False, inferSchema=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a6125211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+--------------------+--------------------+--------------------+--------------------+-------------+--------------+----+--------------+--------------+---------------+----------------+-------------------+------------------+-------------------+--------------------+\n",
      "|   _c0| _c1|                 _c2|                 _c3|                 _c4|                 _c5|          _c6|           _c7| _c8|           _c9|          _c10|           _c11|            _c12|               _c13|              _c14|               _c15|                _c16|\n",
      "+------+----+--------------------+--------------------+--------------------+--------------------+-------------+--------------+----+--------------+--------------+---------------+----------------+-------------------+------------------+-------------------+--------------------+\n",
      "|Header|NULL|voyage_date_voyag...|           voyage_id|voyage_duration_days|voyage_rate_usd_2...|voyage_oh_usd|voyage_hac_usd|NULL|voyage_bcr_usd|voyage_cve_usd|voyage_bnkr_usd|voyage_bnkr_desc|voyage_chtr_usd_bal|voyage_expense_usd|voyage_expense_desc|voyage_bnkr_usd_d...|\n",
      "|  NULL|NULL|           StartDate|    2025-07-03 00:00|    2025-08-12 00:00|    2025-10-04 00:00|         NULL|          NULL|NULL|          NULL|          NULL|           NULL|            NULL|               NULL|              NULL|               NULL|                NULL|\n",
      "|  NULL|NULL|             EndDate|    2025-07-10 00:00|    2025-08-17 00:00|    2025-10-08 00:00|         NULL|          NULL|NULL|          NULL|          NULL|           NULL|            NULL|               NULL|              NULL|               NULL|                NULL|\n",
      "|  NULL|NULL|           Voyage ID|250626/G R/BAL/HA...|250810/GMA/BBA/LS...|250810/GMA/BBA/LS...|         NULL|          NULL|NULL|          NULL|          NULL|           NULL|            NULL|               NULL|              NULL|               NULL|                NULL|\n",
      "+------+----+--------------------+--------------------+--------------------+--------------------+-------------+--------------+----+--------------+--------------+---------------+----------------+-------------------+------------------+-------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f1e99cf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔗 Connected to Supabase schema: voyage table: summary\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)\n",
    "\n",
    "from supabase import create_client\n",
    "\n",
    "# --- 1️⃣ Connect to Supabase ---\n",
    "url = os.getenv(\"SUPABASE_URL\")\n",
    "key = os.getenv(\"SUPABASE_SERVICE_ROLE_KEY\")  # must be service_role key\n",
    "client = create_client(url, key)\n",
    "\n",
    "SCHEMA = \"voyage\"\n",
    "TABLE = \"summary\"\n",
    "\n",
    "print(\"🔗 Connected to Supabase schema:\", SCHEMA, \"table:\", TABLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ef3fe205",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import (\n",
    "    StructType, StructField,\n",
    "    StringType, TimestampType, IntegerType, DecimalType\n",
    ")\n",
    "\n",
    "# Full voyage.summary schema (from your DDL)\n",
    "FULL_SCHEMA = StructType([\n",
    "    StructField(\"updated_at\",           TimestampType(), True),\n",
    "    StructField(\"id\",                   IntegerType(),   False),\n",
    "    StructField(\"voyage_bcr_usd\",       DecimalType(18,2), True),\n",
    "    StructField(\"voyage_date_voyagestart\", TimestampType(), True),\n",
    "    StructField(\"voyage_id\",            StringType(),    True),\n",
    "    StructField(\"voyage_duration_days\", DecimalType(10,4), True),\n",
    "    StructField(\"voyage_rate_usd_24hrgross\", DecimalType(18,2), True),\n",
    "    StructField(\"voyage_oh_usd\",        DecimalType(18,2), True),\n",
    "    StructField(\"voyage_hac_usd\",       DecimalType(18,2), True),\n",
    "    StructField(\"voyage_cve_usd\",       DecimalType(18,2), True),\n",
    "    StructField(\"voyage_bnkr_usd\",      DecimalType(18,2), True),\n",
    "    StructField(\"voyage_bnkr_desc\",     StringType(),    True),\n",
    "    StructField(\"voyage_chtr_usd_bal\",  DecimalType(18,2), True),\n",
    "    StructField(\"voyage_expense_usd\",   DecimalType(18,2), True),\n",
    "    StructField(\"voyage_expense_desc\",  StringType(),    True),\n",
    "    StructField(\"voyage_bnkr_usd_deduc\",DecimalType(18,2), True),\n",
    "    StructField(\"created_by\",           StringType(),    True),  # uuid → keep as text\n",
    "    StructField(\"created_at\",           TimestampType(), True),\n",
    "    StructField(\"updated_by\",           StringType(),    True),  # uuid → keep as text\n",
    "])\n",
    "\n",
    "def schema_for_select_cols(select_cols):\n",
    "    \"\"\"Return a StructType with only the selected columns, in the same order.\"\"\"\n",
    "    # Map field names to fields for quick lookup\n",
    "    field_map = {f.name: f for f in FULL_SCHEMA.fields}\n",
    "    fields = []\n",
    "    for c in select_cols:\n",
    "        if c in field_map:\n",
    "            fields.append(field_map[c])\n",
    "        else:\n",
    "            # If user asked for a non-schema column, treat as string\n",
    "            fields.append(StructField(c, StringType(), True))\n",
    "    return StructType(fields)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd0abec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Found marker 'Header' in column _c0 at row index 0\n",
      "✅ Found marker 'StartDate' in column _c2 at row index 1\n",
      "✅ Found marker 'EndDate' in column _c2 at row index 2\n",
      "✅ Found marker 'Voyage ID' in column _c2 at row index 3\n",
      "1 2 3\n",
      "Header columns (output order): ['voyage_date_voyagestart', 'voyage_id', 'voyage_duration_days', 'voyage_rate_usd_24hrgross', 'voyage_oh_usd', 'voyage_hac_usd', 'voyage_bcr_usd', 'voyage_cve_usd', 'voyage_bnkr_usd', 'voyage_bnkr_desc', 'voyage_chtr_usd_bal', 'voyage_expense_usd', 'voyage_expense_desc', 'voyage_bnkr_usd_deduc']\n",
      "Row indices -> Header: 0 StartDate: 1 EndDate: 2 Voyage ID: 3\n",
      "Discovered 0 set(s):\n",
      "No (voyage_id, date) sets detected under the 'Voyage ID' row. Check your CSV content.\n",
      "✅ Wrote empty file with headers only: C:\\Users\\youngw417/Documents/populate_voyage/db_results.csv\n"
     ]
    }
   ],
   "source": [
    "# ========= CONFIG =========\n",
    "SCHEMA = \"voyage\"\n",
    "TABLE  = \"summary\"\n",
    "DATE_COL = \"voyage_date_voyagestart\"\n",
    "\n",
    "SCHEMA_COLS = {\n",
    "    \"updated_at\",\"id\",\"voyage_bcr_usd\",\"voyage_date_voyagestart\",\"voyage_id\",\n",
    "    \"voyage_duration_days\",\"voyage_rate_usd_24hrgross\",\"voyage_oh_usd\",\"voyage_hac_usd\",\n",
    "    \"voyage_cve_usd\",\"voyage_bnkr_usd\",\"voyage_bnkr_desc\",\"voyage_chtr_usd_bal\",\n",
    "    \"voyage_expense_usd\",\"voyage_expense_desc\",\"voyage_bnkr_usd_deduc\",\"created_by\",\n",
    "    \"created_at\",\"updated_by\"\n",
    "}\n",
    "\n",
    "import re\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "pdf = df.toPandas().fillna(\"\")\n",
    "\n",
    "# ========= HELPERS =========\n",
    "def _norm(s: str) -> str:\n",
    "    s = str(s).strip().casefold()\n",
    "    s = re.sub(r\"\\s+\", \"_\", s)\n",
    "    return re.sub(r\"[^0-9a-z_]+\", \"\", s)\n",
    "\n",
    "def find_marker_row(pdf: pd.DataFrame, marker: str) -> int | None:\n",
    "    \"\"\"\n",
    "    Find the row index where the given marker text appears.\n",
    "    - 'Header' is expected in column A (first column).\n",
    "    - 'StartDate', 'EndDate', 'Voyage ID' are expected in column C (third column).\n",
    "    Returns the matching row index or None if not found.\n",
    "    \"\"\"\n",
    "    # decide which column to search\n",
    "    marker_lower = marker.strip().casefold()\n",
    "    if marker_lower == \"header\":\n",
    "        col_to_check = pdf.columns[0]  # column A\n",
    "    else:\n",
    "        # for StartDate / EndDate / Voyage ID\n",
    "        col_to_check = pdf.columns[2]  # column C\n",
    "\n",
    "    target = _norm(marker)\n",
    "    for i, val in enumerate(pdf[col_to_check]):\n",
    "        if _norm(val) == target:\n",
    "            print(f\"✅ Found marker '{marker}' in column {col_to_check} at row index {i}\")\n",
    "            return i\n",
    "\n",
    "    print(f\"⚠️ Marker '{marker}' not found in column {col_to_check}.\")\n",
    "    return None\n",
    "\n",
    "\n",
    "\n",
    "def parse_to_iso(val, *, is_end: bool=False) -> str|None:\n",
    "    \"\"\"Return ISO 'YYYY-MM-DDTHH:MM:SS'. Returns None if cannot parse.\n",
    "       Date-only → 00:00:00 (start) or 23:59:59 (end) for inclusive range.\"\"\"\n",
    "    if val is None: return None\n",
    "    s = str(val).strip()\n",
    "    if not s or s.upper() == \"NULL\": return None\n",
    "    if s.upper().startswith(\"UTC \"): s = s[4:].strip()\n",
    "    if s.endswith(\"Z\"): s = s[:-1]\n",
    "    s_space = s.replace(\"T\", \" \")\n",
    "    for fmt in (\"%Y-%m-%d %H:%M:%S\",\"%Y-%m-%d %H:%M\",\n",
    "                \"%m/%d/%Y %H:%M:%S\",\"%m/%d/%Y %H:%M\"):\n",
    "        try:\n",
    "            dt = datetime.strptime(s_space, fmt)\n",
    "            return dt.strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
    "        except ValueError:\n",
    "            pass\n",
    "    for fmt in (\"%Y-%m-%d\",\"%m/%d/%Y\"):\n",
    "        try:\n",
    "            d = datetime.strptime(s_space, fmt)\n",
    "            return d.strftime(\"%Y-%m-%dT23:59:59\" if is_end else \"%Y-%m-%dT00:00:00\")\n",
    "        except ValueError:\n",
    "            pass\n",
    "    return None\n",
    "\n",
    "def header_cols_from_header_row_noheaderpdf(pdf: pd.DataFrame) -> list[str]:\n",
    "    \"\"\"With header=False, the 'Header' marker is a DATA row.\n",
    "       Return the DB columns on that row (excluding the literal 'Header' cell).\"\"\"\n",
    "    r_hdr = find_marker_row(pdf, \"Header\")\n",
    "    if r_hdr is None:\n",
    "        raise ValueError(\"Could not find a 'Header' row in the CSV (read with header=False).\")\n",
    "    # Keep all non-empty cells in that row, except the marker cell itself\n",
    "    row_vals = [str(v).strip() for v in pdf.iloc[r_hdr].values if v is not None]\n",
    "    row_vals = [v for v in row_vals if v and _norm(v) != _norm(\"Header\") and v.upper() != \"NULL\"]\n",
    "    # Keep only real DB columns, preserve order\n",
    "    cols = [c for c in row_vals if c in SCHEMA_COLS]\n",
    "    if not cols:\n",
    "        raise ValueError(\"Header row found but no valid DB columns on that row.\")\n",
    "    return cols, r_hdr\n",
    "\n",
    "# ========= 1) Locate rows =========\n",
    "r_hdr = None\n",
    "header_cols, r_hdr = header_cols_from_header_row_noheaderpdf(pdf)\n",
    "r_start = find_marker_row(pdf, \"StartDate\")\n",
    "r_end   = find_marker_row(pdf, \"EndDate\")\n",
    "r_id    = find_marker_row(pdf, \"Voyage ID\")\n",
    "\n",
    "\n",
    "print(\"Header columns (output order):\", header_cols)\n",
    "print(\"Row indices -> Header:\", r_hdr, \"StartDate:\", r_start, \"EndDate:\", r_end, \"Voyage ID:\", r_id)\n",
    "\n",
    "if r_id is None:\n",
    "    raise ValueError(\"Could not find 'Voyage ID' marker row (read with header=False).\")\n",
    "\n",
    "# ========= 2) Build query 'sets' by COLUMN =========\n",
    "# We will look at each column. For that column:\n",
    "#   - pick Start/End from the marker rows (cells at r_start, r_end for this column)\n",
    "#   - gather 1..N voyage_ids from rows BELOW r_id until another marker row or until a blank barrier\n",
    "col_names = list(pdf.columns)\n",
    "n_rows = len(pdf)\n",
    "\n",
    "def is_marker_row(i: int) -> bool:\n",
    "    if i is None:\n",
    "        return False\n",
    "    \n",
    "    # markers to look for (normalized)\n",
    "    markers = {\"header\", \"startdate\", \"enddate\", \"voyageid\"}\n",
    "    \n",
    "    # column A = pdf.columns[0], column C = pdf.columns[2]\n",
    "    val_a = str(pdf.iloc[i][pdf.columns[0]]).strip() if not pd.isna(pdf.iloc[i][pdf.columns[0]]) else \"\"\n",
    "    val_c = str(pdf.iloc[i][pdf.columns[2]]).strip() if not pd.isna(pdf.iloc[i][pdf.columns[2]]) else \"\"\n",
    "    \n",
    "    # normalize and compare\n",
    "    if _norm(val_a) in markers or _norm(val_c) in markers:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "stop_rows = set(x for x in [r_hdr, r_start, r_end, r_id] if x is not None)\n",
    "\n",
    "sets = []\n",
    "for col in col_names:\n",
    "    # dates for this column\n",
    "    s_start = parse_to_iso(pdf.iloc[r_start][col], is_end=False) if r_start is not None else None\n",
    "    s_end   = parse_to_iso(pdf.iloc[r_end][col],   is_end=True)  if r_end   is not None else None\n",
    "\n",
    "    # voyage ids BELOW the 'Voyage ID' row until next marker row (or max +10 rows as a guard)\n",
    "    ids = []\n",
    "    max_probe = min(n_rows, r_id + 11)  # look up to 10 rows below\n",
    "    for r in range(r_id + 1, max_probe):\n",
    "        # stop if we hit another marker row\n",
    "        if r in stop_rows or is_marker_row(r):\n",
    "            break\n",
    "        val = pdf.iloc[r][col]\n",
    "        if val is None: \n",
    "            continue\n",
    "        s_val = str(val).strip()\n",
    "        if not s_val or s_val.upper() == \"NULL\":\n",
    "            continue\n",
    "        # ignore cells that still say 'Voyage ID'\n",
    "        if _norm(s_val) == _norm(\"Voyage ID\"):\n",
    "            continue\n",
    "        ids.append(s_val)\n",
    "\n",
    "    # create one set per voyage id (so a single column can have multiple IDs)\n",
    "    for vid in ids:\n",
    "        sets.append({\"start\": s_start, \"end\": s_end, \"id\": vid})\n",
    "\n",
    "print(f\"Discovered {len(sets)} set(s):\")\n",
    "for i, s in enumerate(sets[:12], 1):\n",
    "    print(f\"  {i:02d}. voyage_id={s['id']}, start={s['start']}, end={s['end']}\")\n",
    "if len(sets) > 12:\n",
    "    print(f\"  ... (+{len(sets)-12} more)\")\n",
    "\n",
    "# If no sets -> nothing to query\n",
    "if not sets:\n",
    "    print(\"No (voyage_id, date) sets detected under the 'Voyage ID' row. Check your CSV content.\")\n",
    "    pd_df = pd.DataFrame(columns=header_cols)\n",
    "    out_csv = os.path.expanduser(\"~/Documents/populate_voyage/db_results.csv\")\n",
    "    pd_df.to_csv(out_csv, index=False, encoding=\"utf-8-sig\")\n",
    "    print(\"✅ Wrote empty file with headers only:\", out_csv)\n",
    "else:\n",
    "    # ========= 3) Build SELECT list for query (add keys for dedupe, but drop them for output) =========\n",
    "    query_select_cols = header_cols.copy()\n",
    "    for must in (\"voyage_id\", DATE_COL):\n",
    "        if must not in query_select_cols:\n",
    "            query_select_cols.append(must)\n",
    "\n",
    "    print(\"Query will SELECT:\", query_select_cols)\n",
    "\n",
    "    # ========= 4) Run Supabase queries (one per set), UNION results =========\n",
    "    all_rows = []\n",
    "    page_size = 2000\n",
    "\n",
    "    # IMPORTANT: use the actual client you created earlier: `supabase = create_client(...)`\n",
    "\n",
    "    for i, s in enumerate(sets, 1):\n",
    "        q = client.schema(SCHEMA).table(TABLE).select(\",\".join(query_select_cols))\n",
    "        if s[\"start\"] is not None:\n",
    "            q = q.gte(DATE_COL, s[\"start\"])   # inclusive lower\n",
    "        if s[\"end\"] is not None:\n",
    "            q = q.lte(DATE_COL, s[\"end\"])     # inclusive upper\n",
    "        q = q.eq(\"voyage_id\", s[\"id\"])\n",
    "\n",
    "        # optional ordering\n",
    "        q = q.order(DATE_COL, desc=True, nullsfirst=False)\n",
    "\n",
    "        page = 0\n",
    "        while True:\n",
    "            resp = q.range(page * page_size, page * page_size + page_size - 1).execute()\n",
    "            batch = resp.data or []\n",
    "            all_rows.extend(batch)\n",
    "            if len(batch) < page_size:\n",
    "                break\n",
    "            page += 1\n",
    "\n",
    "    print(f\"Fetched raw rows (pre-dedup): {len(all_rows)}\")\n",
    "\n",
    "    # ========= 5) Pandas de-dup + OUTPUT in exact header order =========\n",
    "    if all_rows:\n",
    "        pd_df = pd.DataFrame(all_rows)\n",
    "    else:\n",
    "        pd_df = pd.DataFrame(columns=query_select_cols)\n",
    "\n",
    "    # De-dup on unique pair\n",
    "    if {\"voyage_id\", DATE_COL}.issubset(pd_df.columns):\n",
    "        pd_df = pd_df.drop_duplicates(subset=[\"voyage_id\", DATE_COL])\n",
    "    elif \"voyage_id\" in pd_df.columns:\n",
    "        pd_df = pd_df.drop_duplicates(subset=[\"voyage_id\"])\n",
    "    else:\n",
    "        pd_df = pd_df.drop_duplicates()\n",
    "\n",
    "    # Keep ONLY header_cols in EXACT order (even if we fetched keys)\n",
    "    for c in header_cols:\n",
    "        if c not in pd_df.columns:\n",
    "            pd_df[c] = \"\"\n",
    "    pd_df = pd_df[header_cols]\n",
    "\n",
    "    # Write (with temp-rename to avoid Excel locks)\n",
    "    import tempfile, shutil\n",
    "    out_csv = os.path.expanduser(\"~/Documents/populate_voyage/db_results.csv\")\n",
    "\n",
    "    tmp_fd, tmp_path = tempfile.mkstemp(suffix=\".csv\", dir=os.path.dirname(out_csv))\n",
    "    os.close(tmp_fd)\n",
    "    pd_df.to_csv(tmp_path, index=False, encoding=\"utf-8-sig\")\n",
    "    try:\n",
    "        shutil.move(tmp_path, out_csv)\n",
    "        print(\"✅ Wrote:\", out_csv, \"| rows:\", len(pd_df), \"| columns:\", list(pd_df.columns))\n",
    "    except PermissionError:\n",
    "        alt = out_csv.replace(\".csv\", \"_new.csv\")\n",
    "        shutil.move(tmp_path, alt)\n",
    "        print(f\"⚠️ Excel lock detected — saved as '{alt}' instead.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
